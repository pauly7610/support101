import asyncio
import sys
from unittest.mock import AsyncMock, patch

import pytest

from packages.llm_engine.chains.rag_chain import SIMILARITY_THRESHOLD, RAGChain

sys.modules["langchain_openai"] = __import__("types")  # Patch if not installed for CI


@pytest.mark.asyncio
class TestRAGChain:
    @pytest.fixture(autouse=True)
    def patch_env(self, monkeypatch):
        monkeypatch.setenv("PINECONE_API_KEY", "test-key")
        monkeypatch.setenv("PINECONE_ENVIRONMENT", "gcp-starter")
        monkeypatch.setenv("LLM_MODEL_NAME", "gpt-4o")

    @pytest.fixture
    def rag(self):
        with patch(
            "packages.llm_engine.chains.rag_chain.get_fastembed_model", return_value=AsyncMock()
        ), patch("packages.llm_engine.chains.rag_chain.ChatOpenAI", return_value=AsyncMock()):
            yield RAGChain()

    @patch(
        "packages.llm_engine.chains.rag_chain.RAGChain._safe_query_pinecone", new_callable=AsyncMock
    )
    @patch("packages.llm_engine.chains.rag_chain.ChatOpenAI")
    async def test_llm_timeout(self, mock_llm, mock_query, rag):
        # Simulate LLM timeout
        mock_llm().ainvoke.side_effect = asyncio.TimeoutError()
        result = await rag.generate("timeout test")
        assert result["error_type"] == "llm_timeout"
        assert result["retryable"] is True
        assert "exceeded 30s" in result["message"]
        assert result["documentation"].endswith("E429")

    @patch(
        "packages.llm_engine.chains.rag_chain.RAGChain._safe_query_pinecone", new_callable=AsyncMock
    )
    @patch("packages.llm_engine.chains.rag_chain.ChatOpenAI")
    async def test_pinecone_api_error(self, mock_llm, mock_query, rag):
        # Simulate Pinecone API error
        mock_query.side_effect = Exception("PINECONE_API_KEY=secret-key")
        result = await rag.generate("pinecone fail")
        assert result["error_type"] == "vector_store_error"
        assert "***" in result["message"] or "***" in str(result)
        assert result["retryable"] is True

    @patch(
        "packages.llm_engine.chains.rag_chain.RAGChain._safe_query_pinecone", new_callable=AsyncMock
    )
    @patch("packages.llm_engine.chains.rag_chain.ChatOpenAI")
    async def test_citation_filtering(self, mock_llm, mock_query, rag):
        # Simulate Pinecone returns matches above/below threshold
        mock_query.return_value = [
            {
                "score": SIMILARITY_THRESHOLD + 0.01,
                "metadata": {
                    "source_url": "url1",
                    "text": "Relevant doc",
                    "last_updated": "2025-01-01",
                },
            },
            {
                "score": SIMILARITY_THRESHOLD - 0.1,
                "metadata": {
                    "source_url": "url2",
                    "text": "Irrelevant",
                    "last_updated": "2025-01-01",
                },
            },
        ]
        mock_llm().ainvoke.return_value = "Here is an answer."
        result = await rag.generate("filter test")
        # Only one citation should be included
        assert len(result["citations"]) == 1
        assert result["citations"][0]["url"] == "url1"
        assert result["citations"][0]["confidence"] >= SIMILARITY_THRESHOLD

    @patch(
        "packages.llm_engine.chains.rag_chain.RAGChain._safe_query_pinecone", new_callable=AsyncMock
    )
    @patch("packages.llm_engine.chains.rag_chain.ChatOpenAI")
    async def test_empty_context(self, mock_llm, mock_query, rag):
        mock_query.return_value = []
        mock_llm().ainvoke.return_value = "No docs."
        result = await rag.generate("no context")
        assert result["citations"] == []
        assert "No relevant documentation" in result["reply"] or result["reply"]

    @patch(
        "packages.llm_engine.chains.rag_chain.RAGChain._safe_query_pinecone", new_callable=AsyncMock
    )
    @patch("packages.llm_engine.chains.rag_chain.ChatOpenAI")
    async def test_malformed_pinecone_response(self, mock_llm, mock_query, rag):
        # Simulate Pinecone returns malformed data
        mock_query.return_value = [{"bad": "data"}]
        mock_llm().ainvoke.return_value = "Malformed."
        result = await rag.generate("malformed")
        # Should not crash, should return empty citations or error
        assert isinstance(result, dict)
        assert "citations" in result
        assert isinstance(result["citations"], list)
