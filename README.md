ğŸ§  Support Intelligence Core (SIC) â€” Monorepo
A modular, LLM-powered customer support platform designed for rapid deployment and internal extensibility.

Includes:

ğŸš€ FastAPI backend (LangChain RAG, ingestion pipeline, Pinecone, HuggingFace or OpenAI)

ğŸ§‘â€ğŸ’¼ Agent Copilot: Chrome extension that embeds in Zendesk/Intercom

ğŸ’¬ Customer Chatbot: Website widget (Next.js + Tailwind)

ğŸ§© Shared models, chains, telemetry, and design system across all apps

Built for speed, reusability, and modularity â€” to help you build vs. buy with confidence.

ğŸ“ Monorepo Structure
bash
Copy
Edit
support-core/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ backend/           # FastAPI API (RAG, ingestion, LLM)
â”‚   â”œâ”€â”€ agent-copilot/     # React Chrome Extension for agent support
â”‚   â””â”€â”€ customer-bot/      # Next.js Chatbot widget
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ shared/            # Pydantic models, constants, utils
â”‚   â”œâ”€â”€ llm-engine/        # LangChain chains, vector store, prompts
â”‚   â””â”€â”€ observability/     # LangSmith, PromptLayer, OpenTelemetry hooks
â”œâ”€â”€ .env.template
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ turbo.json
â””â”€â”€ README.md (you are here)
ğŸ”‘ Key Features
ğŸ“š Retrieval-Augmented Generation (RAG)
Query embedding + document search via Pinecone

Context-aware generation using HuggingFace or OpenAI LLMs

Source citation for all responses

ğŸ”„ Documentation Ingestion
Ingest content from public URLs (e.g., Firecrawl-ready)

Chunk, embed, and store content with /ingest_documentation

Markdown & semantic chunking support

ğŸ§‘â€ğŸ’» Agent Copilot (Chrome Extension)
Injected into Zendesk or Intercom UI

Auto-detects customer query or lets agent paste it in

Shows suggested reply + source documents

Easy copy-paste to reply

ğŸ’¬ Customer Chatbot
Embeddable floating widget

Asks user questions â†’ backend RAG â†’ shows instant answers

Cites doc links for full context

ğŸ›  Shared Infrastructure
Shared Pydantic models for contracts across frontend/backend

Shared telemetry via LangSmith + PromptLayer

Modular LangChain chains for RAG and memory

Unified UI design system (via DESIGN_SYSTEM.md)

âš¡ Quickstart
1. Clone + Setup Environment
bash
Copy
Edit
git clone https://github.com/pauly7610/support101
cd support101
cp .env.template .env
Fill in values for:

PINECONE_API_KEY

FIRECRAWL_API_KEY

HUGGINGFACE_API_KEY (or OpenAI)

LANGSMITH_API_KEY, etc.

2. Install Dependencies
Backend

bash
Copy
Edit
cd apps/backend
pip install -r requirements.txt
Frontends

bash
Copy
Edit
cd apps/agent-copilot && npm install
cd apps/customer-bot && npm install
3. Run Locally
Backend

bash
Copy
Edit
uvicorn apps.backend.main:app --reload
Agent Copilot Extension

bash
Copy
Edit
cd apps/agent-copilot
npm run dev
Customer Bot Widget

bash
Copy
Edit
cd apps/customer-bot
npm run dev
4. Test It Out
Visit a helpdesk page with the extension running â€” see the Copilot sidebar

Open the website widget â†’ ask a question

Both hit /generate_reply â†’ get grounded answers with source docs

ğŸ“¡ API Endpoints
Method	Route	Description
GET	/health	Simple health check
POST	/generate_reply	Main endpoint for LLM reply generation
POST	/ingest_documentation	Crawl & embed new doc content

ğŸ§  Developer Notes
See individual app README.mds for dev details

Uses Turborepo for task orchestration

Docker support in docker-compose.yml (coming soon)

Add new chains or document loaders in packages/llm-engine

Extend observability in packages/observability

ğŸš€ Deployment
 Railway, Render, or AWS-compatible with Docker

 Add CI via GitHub Actions (lint/test/build)

 Staging + production config via env vars

ğŸ“ Resources
DESIGN_SYSTEM.md: Shared UI guidelines + tokens

packages/shared: Source of truth for all models/types

turbo.json: Task graph for multi-app orchestration

