ğŸ§  Support Intelligence Core (SIC) â€” Product Requirements Document (MVP)
TL;DR
The Support Intelligence Core (SIC) is an LLM-powered platform designed to modernize customer support through an Agent Copilot and Customer Chatbot. The MVP will ingest company documentation, process user queries via a RAG pipeline with open-source LLMs, and deliver intelligent responses to both agents and customers. It reduces support costs, improves efficiency, and lays the groundwork for advanced AI capabilities.

ğŸ¯ Goals & Objectives
ğŸ¥‡ Primary Goals
Enhance Agent Efficiency: Reduce Average Handle Time (AHT); increase ticket throughput.

Improve Customer Satisfaction (CSAT): Faster, more consistent responses via Copilot & Chatbot.

Reduce Support Costs: Deflect common issues via self-service; speed agent workflows.

ğŸ¥ˆ Secondary Objectives
Improve knowledge accessibility for both agents and customers.

Ensure standardized, consistent answers to FAQs.

Support new agent onboarding with real-time coaching via Copilot.

Build modular, extensible AI infrastructure for future features.

ğŸ‘¤ Target Users & Personas
ğŸ§ Sarah, Tier 1 Agent
Needs: Fast KB access, better context, response drafting help.

Pain: Too many tabs, slow answers, repetitive questions.

SIC Value: Real-time suggestions, fewer clicks, faster resolutions.

ğŸ’¬ John, Product User
Needs: Quick, accurate answers; no agent wait.

Pain: Long wait times, poor search UX on help sites.

SIC Value: AI-powered chatbot with relevant, contextual links.

ğŸ›  Mike, Support Manager
Needs: Easy KB maintenance, visibility into AI effectiveness.

Pain: Outdated docs, no analytics, retraining headaches.

SIC Value: (MVP) Simple ingestion pipeline; future: analytics, tuning UI.

ğŸ§© Key Features & User Stories (MVP Scope)
ğŸ§  Core Backend & LLM Engine
F-BE-001 â€” Documentation Ingestion & Processing
Crawl specified URLs

Extract + chunk content

Embed using HuggingFace

Store in Pinecone

F-BE-002 â€” RAG Chain
Embed user query

Search Pinecone

Retrieve top N chunks

Generate LLM response using context

Return response + source URLs

F-BE-003 â€” Analytics & Compliance
- Store escalation analytics in PostgreSQL
- Provide dashboard with filters (user, date range)
- Expose Prometheus metrics (LLM response times, API errors)
- Ensure all analytics endpoints have error handling and unified error responses
- Data privacy: Anonymize logs after 30 days, mask API keys, GDPR/CCPA endpoints
- Accessibility: ARIA labels, keyboard navigation, color contrast â‰¥4.5:1

F-BE-004 â€” API for Frontend Consumption
POST /generate_reply endpoint

Accepts TicketContext (query, chat history)

Returns SuggestedResponse (text + sources)

/health endpoint

API key auth optional

ğŸ§‘â€ğŸ’» Agent Copilot (React Chrome Extension)
F-AC-001 â€” Helpdesk Sidebar Injection
Inject into Zendesk/Intercom

Sidebar displays next to tickets

F-AC-002 â€” Suggested Replies
Detect customer message or allow paste-in

Display AI-suggested response

Copy to clipboard with 1 click

F-AC-003 â€” Show Source Documents
Show list of document titles/URLs

Click to open full context

ğŸ—£ Customer Chatbot (Next.js + Tailwind Widget)
F-CB-001 â€” Basic Chat Interface
Floating button â†’ chat window

Input box for user queries

Chat history display

F-CB-002 â€” Process & Display Responses
Capture question â†’ call API â†’ show answer

F-CB-003 â€” Source Document Links
Display cited docs (titles + links)

Open in new tab

ğŸ” Shared Components & Observability
F-SH-001 â€” Shared Pydantic Models
Centralized schema for TicketContext, SuggestedResponse, etc.

F-OB-001 â€” Observability
LangSmith + PromptLayer: track LLM queries + chain runs

OpenTelemetry integration for backend

âš™ï¸ Technical Stack
Backend: FastAPI, LangChain, HuggingFace Embeddings, Pinecone

LLM: Mistral 7B / LLaMA 3 via wrapper

Frontend:

Copilot: React Chrome Extension (Zendesk/Intercom)

Chatbot: Next.js + Tailwind

Observability: LangSmith, PromptLayer, OpenTelemetry

Monorepo Tooling: Turborepo

DevOps: Docker, Railway or Render

ğŸ“ˆ Success Metrics (MVP)
ğŸ§‘â€ğŸ’¼ Agent Efficiency
Qualitative feedback from pilot agents

(Post-MVP): AHT reduction, increased ticket throughput

ğŸ‘¥ Chatbot Effectiveness
Number of resolved queries without escalation

(Post-MVP): Deflection rate from live agents

âš™ï¸ System Performance
/generate_reply response time < 5s (Agent), < 3s (Customer)

Successful document ingestion rate

ğŸš€ Adoption
% of agents who continue using Copilot post-test

Chatbot usage on test site

ğŸ§ª Testing & Quality
- Manual E2E tests for Copilot/Chatbot flows
- Linting: Black, Flake8, ESLint, Prettier
- CI: GitHub Actions, required checks
- Async DB mocking for all backend tests (pytest-asyncio)
- 100% coverage for compliance and analytics endpoints

ğŸ§ª Non-Functional Requirements (NFRs)
Performance: <5s agent reply, <3s chatbot

Scalability: Stateless API, scalable vector DB

Availability: 99.5% uptime target

Maintainability: Modular codebase, env-based config

Security: Secret-managed keys, PII awareness

Modularity: Abstract LLM, RAG, and vector store

ğŸ”­ Future Considerations (Post-MVP)
Sentiment detection, summarization, proactive Copilot suggestions

Memory-based conversational chatbot with escalation

Multi-language support, voice I/O

Admin dashboard: ingestion config, analytics

Richer analytics + observability

More sources: PDFs, Confluence, Notion

LLM fine-tuning pipelines

ğŸš« Out of Scope (MVP)
Persistent memory/chat history

Auth/SSO for frontend

Admin dashboard

Voice, real-time collab, live-agent handoff

â“ Open Questions
What exact documentation URLs should we ingest first?

Zendesk or Intercom as first Copilot target?

What qualifies a chatbot interaction as "resolved"?

Any PII in the initial docs?

